<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      Code Compression Summary | Neural Notes
    
  
</title>
<meta name="author" content="Neural Notes">
<meta name="description" content="Notes on Code Compression Summary">

  <meta name="keywords" content="machine-learning, deep-learning, langchain, langgraph, notes">






  <!-- OpenGraph -->
  <meta property="og:site_name" content="Neural Notes">
  <meta property="og:type" content="article">
  <meta property="og:title" content="Neural Notes | Code Compression Summary">
  <meta property="og:url" content="https://krishan098.github.io/notes/blog/2024/code-compression-summary/">
  <meta property="og:description" content="Notes on Code Compression Summary">
  
  <meta property="og:locale" content="en">

  <!-- Twitter card -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Code Compression Summary">
  <meta name="twitter:description" content="Notes on Code Compression Summary">
  
  



  <!-- Schema.org -->
  
  
  

  <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Neural Notes"
        },
        "url": "https://krishan098.github.io/notes/blog/2024/code-compression-summary/",
        "@type": "BlogPosting",
        "description": "Notes on Code Compression Summary",
        "headline": "Code Compression Summary",
        
        "sameAs": ["https://github.com/Krishan098"],
        
        "name": "Neural Notes",
        "@context": "https://schema.org"
    }
  </script>



<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/notes/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">



<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/notes/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" href="/notes/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772">
<link defer rel="stylesheet" type="text/css" href="">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/notes/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">


  <!-- Sidebar Table of Contents -->
  <link defer href="/notes/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet">


<!-- Styles -->




  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9D&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/notes/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="https://krishan098.github.io/notes/blog/2024/code-compression-summary/">


  <!-- Dark Mode -->
  <script src="/notes/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script>
  <link defer rel="stylesheet" href="/notes/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>










  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/notes/">
          Neural Notes
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/notes/">About
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item active">
                  <a class="nav-link" href="/notes/blog/">blog
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
          
            <!-- Search -->
            <li class="nav-item">
              <button id="search-toggle" title="Search" onclick="openSearchModal()">
                <span class="nav-link">ctrl k <i class="ti ti-search"></i></span>
              </button>
            </li>
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        
          <div class="row">
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-3">
              <nav id="toc-sidebar" class="sticky-top"></nav>
            </div>
            <!-- main content area -->
            <div class="col-sm-9">






<div class="post">
  <header class="post-header">
    <h1 class="post-title">Code Compression Summary</h1>
    <p class="post-meta">
      Created on January 09, 2024
      
      
      
    </p>
    <p class="post-tags">
      
        <a href="/notes/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>
      
      
          ·  
        
          
            <a href="/notes/blog/tag/code-compression"> <i class="fa-solid fa-hashtag fa-sm"></i> code-compression</a>
          
          
        
      

      
          ·  
        
          
            <a href="/notes/blog/category/code-compression"> <i class="fa-solid fa-tag fa-sm"></i> Code Compression</a>
          
          
        
      
    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <h1 id="orchestrating-structure-and-efficiency-in-llm-contexts">Orchestrating Structure and Efficiency in LLM contexts</h1>

<ul>
  <li>
    <p><strong>AST-T5</strong>: enhances code understanding by leveraging <strong>Abstract Syntax Tree</strong> structures without complex architectural changes, leading to superior parameter efficiency.</p>
  </li>
  <li>
    <p><strong>Graph Retrieal-Augmented Generation for Customized LLM</strong>: It revolutionizes LLM customization for specialised domains by utilizing graph-structured knowledge representation for multi-hop reasoning and enhanced knowledge retrieval, improving interpretability and token efficiency.</p>
  </li>
  <li>
    <p><strong>In-Context Former:</strong> An efficient context compression model that dramatically reduces LLM inference costs with a linear time complexity by using learnable “digit tokens” and operating independently of the LLM.</p>
  </li>
</ul>

<h2 id="ast-t5-structure-aware-pretraining-for-code-generation-and-understanding">AST-T5: Structure-Aware Pretraining for Code Generation and Understanding</h2>

<ul>
  <li>
    <p>AST-T5 is a novel pretraining paradigm designed to enhance large language models for code-related tasks by leveraging Abstract Syntax Tree structure of code. Traditional LLMs treat code as simple sequences, ignoring its inherent structure, which limits their performance.</p>
  </li>
  <li>
    <p>ASTs could improve performance but are computationally expensive.</p>
  </li>
  <li>
    <p>AST-T5 overcomes these limitations by integrating AST awareness without complex program analysis or architectural changes, making it seamlessly compatible with any encoder-decoder Transformer, similar to Vanilla T5.</p>
  </li>
  <li>
    <p>The core contributions are:</p>
    <ul>
      <li>
        <p><strong>AST-Aware Segmentation</strong>: This dynamic programming-based algorithm is used to split lengthy code files into chunks while preserving the structural integrity of the code, minimizing disruptions to AST structures. This is a significant improvement over “Greedy Segmentation”.</p>
      </li>
      <li>
        <p><strong>AST-Aware Span Corruption:</strong>It specifically masks code spans that correspond to AST subtrees, ranging from individual tokens to entire function bodies. This trains the model to reconstruct coherent code structures. <strong>theta:</strong> hyperparameter that controls the granularity of masking, enabling diverse training scenarios, from single-token completion to full function body generation.</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="interesting-fact">Interesting Fact:</h3>
<ul>
  <li>
    <p>high performance gains through data preparation and masking strategies, rather than complex architectural changes or computationally expensive analysis.</p>
  </li>
  <li>
    <p>Granularity control for masking</p>
  </li>
  <li>
    <p>Empirical Component Analysis: increasing the mask ratio improved generation without compromising understanding tasks.</p>
  </li>
  <li>
    <p>Parameter Efficiency: only 277M parameters</p>
  </li>
</ul>

<h3 id="identified-gaps">Identified gaps:</h3>
<ul>
  <li>
    <p>Natural language performance: specialised code masking might lead to suboptimal performance in natural language generation.</p>
  </li>
  <li>
    <p>Syntactically Invalid code: the method assumes syntactically valid for parsing.</p>
  </li>
</ul>

<h3 id="methodology-thinking-inspiration">Methodology Thinking Inspiration</h3>

<ul>
  <li>
    <p>Domain-specific structure is Gold: This paper strongly represents that explicitly incorporating domain-specific structural knowledge is often more effective than treating inputs as flat sequences.</p>
  </li>
  <li>
    <p>Smart Data Preparation: The dynamic programming approach for structure-preserving data segmentation is a valuable technique that could be generalised to other structured data types to optimize inputs for LLMs.</p>
  </li>
  <li>
    <p>Targeted Pretraining Objectives: Designing pretraining objectives that directly align with the desired output structure is a powerful way to guide model learning.</p>
  </li>
  <li>
    <p>The paper acknowledges that AST-leveraging methods “necessitate parsing or static analysis for downstream tasks, which is less feasible for incomplete or incorrect code scenarios. “</p>
  </li>
  <li>
    <p>yes , AST-T5’s pretraining objective and architecture are compatible with Vanilla T5. It can seamlessly integrate as a drop-in replacement for any T5 variant, suggesting it can leverage existing T5 model architectures without requiring complete re-training of the base model.</p>
  </li>
  <li>
    <p>An AST is a tree representation of the syntactic structure of code. Each node in the tree represents a construct from the source code (e.g., variable, loop, function call).</p>
  </li>
</ul>

<hr>

<h2 id="a-survey-of-graph-retrieval-augmented-generation-for-customised-large-language-models">A survey of Graph Retrieval Augmented Generation for customised Large Language Models</h2>

<ul>
  <li>
    <p>This survey introduces Graph-based Retrieval-Augmented Generation(GraphRAG) as a revolutionary paradigm for customizing LLMs for specialised domains.</p>
  </li>
  <li>
    <p>Traditional LLMs struggle with domain-specific knowledge due to limitations in knowledge depth, reasoning complexity, and contet sensitivity. Standard RAG systems rely on flat text retrieval, face challenges with complex query understanding, integrating distributed knowledge, ontxt window limitations and scalbility bottlenecks.</p>
  </li>
  <li>
    <p>GraphRAG addresses these through 3 key innovations:</p>
  </li>
</ul>

<ol>
  <li>
    <p><strong>Graph-structured knowledge representation:</strong> It explicitly captures entity relationships and domain hierarchies using Knowledge graphs.</p>
  </li>
  <li>
    <p><strong>Efficient graph-based retrieval techniques:</strong> It enables context-preserving knowledge retrieval with multi-hop reasoning capabilities by traversing graph structures.</p>
  </li>
  <li>
    <p><strong>Structure-aware knowledge integration algorithms:</strong> It leverages retrieved knowledge for accurate and logically coherent LLM generation.</p>
  </li>
</ol>

<ul>
  <li>
    <p>The survy categorizes existing GraphRAG models into 3 main paradigms for knowledge organisation:</p>
  </li>
  <li>
    <p><strong>Knowledge-based GraphRAG(Graph as knowledge Carriers):</strong> Transforms unstructured text into explicit, structured KGs, where nodes represent concepts and edges capture semantic relationships. This enables better representation of hierarchical relationships and complex knowledge dependencies, facilitating logic-guided chain retrieval and multi-step reasoning.</p>
  </li>
  <li>
    <p><strong>Index-based GraphRAG(Graph for knowledge indexing):</strong> maintains the original textual form but uses graph structures as an indexing mechanism to organise and retrieve relevant text chunks efficiently.</p>
  </li>
  <li>
    <p><strong>Hybrid GraphRAG:</strong> Combines the strengths of both, using graphs as carriers of key information while linking to original text chunks for detailed contextual information.</p>
  </li>
  <li>
    <p>The knowledge retrieval process in GraphRAG involves query/graph preprocessing, matching(semantic similarity,structural relationships), and knowledge pruning to refine retrieved subgraphs. Various retrieval techniques are employed, including semantic-similarity based, logical reasoning based, GNN based,LLm-based and RL based retrievers.</p>
  </li>
  <li>
    <p>Retrieval can also be enhanced through multi-round retrieval, post-retrieval validation, and hybrid retrieval from multiple sources.</p>
  </li>
  <li>
    <p>For <strong>Knowledge integration</strong>, GraphRAG uses either fine-tuning(injecting node-level,path-level or subgraph-level knowledge into opensource LLMs) or in-context learning(for closed-source LLMs, using graph-enhanced Chain-of-Thought or collaborative KG refinement via prompt engineering)</p>
  </li>
  <li>
    <p>GraphRAG offers several advantages over traditional RAG: enhanced knowledge representation, flexibility in knowledge sources, improved efficiency and scalability and better interpretability through traceable reasoning paths.</p>
  </li>
  <li>
    <p>Built-in Interpretability: The inherent transparency of GraphRAG, allowing users to “trace the path of reasoning” through the knowledge graph</p>
  </li>
  <li>
    <p>Token Efficiency Claim: The claim that GraphRAG systems can achieve LLM responses with 26% to 97% fewer tokens than traditional methods is highly significant for reducing computational costs and latency.</p>
  </li>
</ul>

<h3 id="limitations">Limitations:</h3>

<ul>
  <li>
    <p><strong>Lack of high quality KGs</strong>: scarcity of high quality, comprehensive KGs, especially for new or niche domain.</p>
  </li>
  <li>
    <p><strong>KG construction Trade-offs:</strong> When building KGs from text, there’s a tradeoff between retaining fine-grained detail and compactness. LLM-based summarization for KG construction can also be computationally expensive.</p>
  </li>
  <li>
    <p><strong>LLM’s Graph-structured input challenge:</strong> LLMs are not inherently designed to process graph-structured data and often require conversion to natural language.</p>
  </li>
  <li>
    <p><strong>Error accumulation:</strong> In-context learning methods like Graph-enhanced Chain-of-Thought still face the problem of error accumulation in multi-step reasoning.</p>
  </li>
  <li>
    <p>For a new GraphRAG we can start by using Open Information Extraction techniques to extract relational facts and triples from unstructured domain-specific texts.</p>
  </li>
</ul>

<h3 id="addressing-fixed-context-window-limitation">Addressing fixed context window limitation</h3>

<ul>
  <li>While token reduction is part of it, it provides more concise,relevant and distillled information. Instead of providing large, potentially noisy text chunks, it retrieves pre-pruned factual information or specific subgraphs directly relevant to the query. This allows the LLM to fit more essential, semantically rich knowledge into its context window, facilitating multi-hop reasoning within that compressed and structured context.</li>
</ul>

<h3 id="knowledge-based-graphrag-vs-index-based-graphrag">knowledge-based GraphRAG vs Index-based GraphRAG</h3>

<ul>
  <li>
    <p>Knowledge-based GraphRAG is preferred when deep, precise, multi-step reasoning and explicit semantic relationships are crucial, as it transforms text into structured KGs that are highly amenable to logical inference and auditing.</p>
  </li>
  <li>
    <p>Index-based GraphRAG is better when the primary goal is efficient retrieval of raw, detailed textual information while still benefitting from some graph-based organization to establish semantic connections between text chunks. It’s often used when maintaing the original text is important for context.</p>
  </li>
  <li>
    <p>Knowledge-based GraphRAG treats graphs as “knowledge carriers” by transforming unstructured text into explicit, structured knowledge graphs (KGs), where nodes are concepts and edges are semantic relationships. Index-based GraphRAG, conversely, uses graphs as “indexing tools” by maintaining the original textual form but employing graph structures to organize and efficiently retrieve relevant text chunks.</p>
  </li>
</ul>

<h2 id="in-context-former">In-context former</h2>

<ul>
  <li>
    <p>In-context Former is a novel and highly efficient context compression model designed to significantly reduce the high infernece costs of Transformer-based LLMs, which typically suffer from quadratic time complexity due to their self attention mechanism with long input contexts.</p>
  </li>
  <li>
    <p>operates independently of the LLM</p>
  </li>
  <li>
    <p>uses a cross-attention mechanism and a small fixed number of learnable “digest tokens”. These digest tokens act as queries that directly condense information from the contextual word embeddings(keys and values) into compact “digest vectors”.</p>
  </li>
  <li>
    <p>This achieves a linear time complexity w.r.t context length n.</p>
  </li>
  <li>
    <p>Rotational positional embeddings are applied to ensure the model captures are applied to ensure the model captures positional relationships within the context.</p>
  </li>
</ul>

<h3 id="involves-2-phases">Involves 2 phases:</h3>

<ol>
  <li>
    <p><strong>Pretraining</strong>: The odel is trained to generate digest vectors such that a fixed LLM can accurately reconstruct the original context from these vectors, forcing IC-former to preserve essential contextual information.</p>
  </li>
  <li>
    <p><strong>Instruction Fine-tuning:</strong> IC-Former is further fine-tuned on instruction data to ensure that the generated digested vectors enable the LLM to correctly respond to various context-related prompts. For long contexts , a divide and conquer strategy is employed, where contexts are split into manageable chunks, individually compressed and then concatenated.</p>
  </li>
</ol>

<ul>
  <li>
    <p>It requires only 1/32 of the floating-point operations (FLOPs) compared to a baseline (ICAE) for 4x compression</p>
  </li>
  <li>
    <p>It achieves compression speeds 68 to 112  times faster than the baseline</p>
  </li>
  <li>
    <p>Analysis of attention maps reveals that IC-Former compresses context by aggregating information from adjacent tokens in sequential order, with different layers focusing on different grammatical categories.</p>
  </li>
  <li>
    <p>Model Decoupling: The ability to operate as a separate, lightweight compressor that does not modify or depend on the target LLM is highly valuable.</p>
  </li>
</ul>

<h3 id="limitations-1">Limitations:</h3>

<ul>
  <li>
    <p>The paper explicitly mentions resource constraints preventing testing on larger models and longer contexts, which limits the full validation of its scalability</p>
  </li>
  <li>
    <p>Sub-baseline Performance: While “over 90%,” the fact that it “has not surpassed the baseline’s performance in downstream tasks” implies a slight quality trade-off for the massive speed gains. This might not be acceptable for all high-accuracy critical applications.</p>
  </li>
</ul>

<h3 id="inspiration">inspiration</h3>

<ul>
  <li>Modular AI Systems: The success of the decoupled compressor highlights the benefits of a modular design in LLM pipelines, where specialized components handle specific tasks (like compression) without affecting the core LLM.</li>
</ul>

<h2 id="evaluation">EVALUATION</h2>

<ul>
  <li>The primary method for assessing the correctness of generated code across multiple papers is through execution on unit tests. Benchmarks like HumanEval, MBPP, and APPS measure performance using metrics such as “Pass@1,” which indicate whether the generated code successfully passes a set of provided test cases</li>
</ul>

    </div>
  </article>

  

  

  
    
  

  
  
</div>
</div>
          </div>
        
      
    </div>

    <!-- Footer -->
    


  <footer class="sticky-bottom mt-5" role="contentinfo">
    

    <div class="container">
      
  © Copyright 2025
  Neural
  
  Notes. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.

  
  
    Last updated: November 27, 2025.
  

    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="/notes/assets/js/bootstrap.bundle.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>


  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="" integrity="" crossorigin="anonymous"></script>
  <script defer src="/notes/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script>























  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/notes/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>



  <!-- Sidebar Table of Contents -->
  <script defer src="/notes/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script>




<!-- Load Common JS -->
<script src="/notes/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/notes/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script>
<script defer src="/notes/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/notes/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>

<!-- Badges -->




  <!-- MathJax -->
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
  
    <script src="/notes/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script>
    <script defer src="" crossorigin="anonymous"></script>
  









  <!-- Scrolling Progress Bar -->
  <script defer src="/notes/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script>







  <!-- Back to Top -->
  <script src="/notes/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>



  <!-- Search -->
  <script type="module" src="/notes/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script>
  <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys>
  <script src="/notes/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script>
  <script src="/notes/assets/js/search-data.js"></script>
  <script src="/notes/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script>




  </body>
</html>
