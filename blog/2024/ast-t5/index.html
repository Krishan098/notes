<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      AST-T5 | Neural Notes
    
  
</title>
<meta name="author" content="Neural Notes">
<meta name="description" content="Notes on AST-T5">

  <meta name="keywords" content="machine-learning, deep-learning, langchain, langgraph, notes">






  <!-- OpenGraph -->
  <meta property="og:site_name" content="Neural Notes">
  <meta property="og:type" content="article">
  <meta property="og:title" content="Neural Notes | AST-T5">
  <meta property="og:url" content="https://krishan098.github.io/notes/blog/2024/ast-t5/">
  <meta property="og:description" content="Notes on AST-T5">
  
  <meta property="og:locale" content="en">

  <!-- Twitter card -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="AST-T5">
  <meta name="twitter:description" content="Notes on AST-T5">
  
  



  <!-- Schema.org -->
  
  
  

  <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Neural Notes"
        },
        "url": "https://krishan098.github.io/notes/blog/2024/ast-t5/",
        "@type": "BlogPosting",
        "description": "Notes on AST-T5",
        "headline": "AST-T5",
        
        "sameAs": ["https://github.com/Krishan098"],
        
        "name": "Neural Notes",
        "@context": "https://schema.org"
    }
  </script>



<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/notes/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">



<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/notes/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" href="/notes/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772">
<link defer rel="stylesheet" type="text/css" href="">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/notes/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">


  <!-- Sidebar Table of Contents -->
  <link defer href="/notes/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet">


<!-- Styles -->




  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9D&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/notes/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="https://krishan098.github.io/notes/blog/2024/ast-t5/">


  <!-- Dark Mode -->
  <script src="/notes/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script>
  <link defer rel="stylesheet" href="/notes/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>










  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/notes/">
          Neural Notes
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/notes/">About
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item active">
                  <a class="nav-link" href="/notes/blog/">blog
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
          
            <!-- Search -->
            <li class="nav-item">
              <button id="search-toggle" title="Search" onclick="openSearchModal()">
                <span class="nav-link">ctrl k <i class="ti ti-search"></i></span>
              </button>
            </li>
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        
          <div class="row">
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-3">
              <nav id="toc-sidebar" class="sticky-top"></nav>
            </div>
            <!-- main content area -->
            <div class="col-sm-9">






<div class="post">
  <header class="post-header">
    <h1 class="post-title">AST-T5</h1>
    <p class="post-meta">
      Created on January 08, 2024
      
      
      
    </p>
    <p class="post-tags">
      
        <a href="/notes/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>
      
      
          ·  
        
          
            <a href="/notes/blog/tag/code-compression"> <i class="fa-solid fa-hashtag fa-sm"></i> code-compression</a>
          
          
        
      

      
          ·  
        
          
            <a href="/notes/blog/category/code-compression"> <i class="fa-solid fa-tag fa-sm"></i> Code Compression</a>
          
          
        
      
    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <h1 id="ast-t5-structure-aware-pretraining-for-code-generation-and-understaning">AST-T5: Structure-Aware Pretraining for Code Generation and Understaning</h1>

<h2 id="abstract">Abstract</h2>

<ul>
  <li>
    <p>Large language models have made significant advancements in code-related tasks, yet many of them treat code as simple sequences, neglecting its structured nature.</p>
  </li>
  <li>
    <p>This paper introduces AST-T5, a novel pretraining paradigm that leverages the <em>Abstract Syntax Tree</em> for enhanced code generation, transpilation and understanding.</p>
  </li>
  <li>
    <p><strong>AST-Aware segmentation</strong> retains code structure using dynamic programming and <strong>AST-Aware Span Corruption</strong> objective equips the model to reconstruct various code structures.</p>
  </li>
  <li>
    <p>Unlike other models, AST-T5 avoids complex program analysis or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer.</p>
  </li>
</ul>

<h2 id="introduction">Introduction</h2>

<ul>
  <li>
    <p>Despite impressive advancements, most models interpret code as mere sequences of subword tokens, over looking its intrinsic structured nature.</p>
  </li>
  <li>
    <p>AST-T5 uses a lightweight, multi-language parser called Tree-sitter, this approach has broad applicability across all syntactically well-defined programming languages.</p>
  </li>
  <li>
    <p>After parsing code into ASTs, it uses a dynamic programming based segmentation algorithm for AST-aware code segmentation to maintain the structural integrity of the input code. Using AST-Aware Span Corruption, the model is pretrained to reconstruct various code structures, ranging from individual tokens to entire function bodies.</p>
  </li>
  <li>
    <p>Together, the approach offers three key advantages:</p>

    <ol>
      <li>
        <p>enriched bidirectional encoding for improved code understanding.</p>
      </li>
      <li>
        <p>the ability to coherently generate code structures</p>
      </li>
      <li>
        <p>a unified, structure-aware pretraining framework that boosts performance across a variety of code-related tasks, particularly in code transpilation.</p>
      </li>
    </ol>
  </li>
</ul>

<p><img src="/notes/assets/img/posts/image-6.png" alt=""></p>

<ul>
  <li>
    <p>In addition, other than the specialized AST-aware masking approach, AST-T5 introducs no architecture changes or additional heads. This compatibility enables seamless integration of our model as a drop-in replacement for any T5 variant.</p>
  </li>
  <li>
    <p>The inherent AST-awareness of AST-T5 offers unique advantages in structure-sensitive tasks, such as code-to-code transpilation and Clone Detection, highlighting its effectiveness at capturing the structural nuances of code.</p>
  </li>
  <li>
    <p>AST-T5 aligns with methods that utilize code structure only in pretraining.</p>
  </li>
</ul>

<h2 id="method">Method</h2>

<ul>
  <li>
    <p>First, AST-T5 parses code into ASTs to enable a deeper understanding of code structure.</p>
  </li>
  <li>
    <p>Leveraging this structure, this paper introduces AST-Aware Segmentation, an algorithm designed to address Transformer token limits while retaining the semantic coherence of the code.</p>
  </li>
  <li>
    <p>Second, it introduces AST-Aware Span Corruption, a masking technique that pretrains AST-T5 to reconstruct code structures ranging from individual tokens to entire function bodies, enhancing both its flexibility and structure-awareness.</p>
  </li>
</ul>

<h3 id="parsing-code-into-asts">Parsing Code into ASTs</h3>

<ul>
  <li>
    <p>Unlike traditional language models on code that handle code as simple sequences of subword ttokens, AST-T5 leverages the Abstract Syntax Tree of code to gain semantic insights.</p>
  </li>
  <li>
    <p>For parsing purpose, it assumes the provided code is syntactically valid- a reasonable assumption for tasks like code transpilation and understanding.</p>
  </li>
  <li>
    <p>It only requires the code to be parsable. It uses a tree sitter , a multi-language parser, to construct the ASTs, where each subtree represents a consecutive span of subword tokens, and every leaf node represents an individual token.</p>
  </li>
</ul>

<h3 id="ast-aware-segmentation">AST-Aware Segmentation</h3>

<ul>
  <li>
    <p>splits lengthy code files into chunks in a structure-preserving manner.</p>
  </li>
  <li>
    <p><strong>Segmentation in language model pretraining</strong> is a critical yet often overlooked aspect. Transformer LMs impose token limits on input sequences, making segmentation essential for fitting these inputs within the max_len constraint. A naive approach is greedy segmentation where each chunk, except the last, contains exactly max_len tokens.</p>
  </li>
  <li>
    <p>Research in NLP underscores that segmentation respecting sentence and document boundaries outperforms the greedy strategy. Given programming language’s inherently structured nature, a more sophisticated segmentation approach is even more important.</p>
  </li>
  <li>
    <p><strong>AST-Aware segmentation</strong> is designed to preserve the AST structure of code during segmentation. Unlike greedy segmentation, which can indiscriminately fragment AST structures, This method strategically minimizes such disruptions.</p>
  </li>
</ul>

<h4 id="dynamic-programming-based-algorithm">Dynamic programming based Algorithm</h4>

<ol>
  <li>
    <p>We construct an array cost, where cost[i] denotes the number of AST-structure breaks that would occur if partitioning happened right after token i. This array is populated by traversing the AST and incrementing cost[1..r-1] by 1 for each span[l,r] associated with an AST subtree.</p>
  </li>
  <li>
    <p>We define a 2-D array dp, where dp[k,i] represents the minimum total number of AST-structure breaks when k partitions are made for the first i tokens, ending the last partition right after the i-th token. The state transition equation is :</p>
  </li>
</ol>

\[dp[k,i]=cost[i]+ min_{\text{i-max_leng&lt;=j&lt;i}}dp[k-1,j]\]

<p><img src="/notes/assets/img/posts/image-7.png" alt=""></p>

<p><img src="/notes/assets/img/posts/image-8.png" alt=""></p>

<ol>
  <li>
    <p>While the naive DP algorithm has a quadratic time complexity $O(n^2)$ relative to the code file length n, it can be optimized to $O(n^2/max_len)$ by employing a monotonic queue for sliding-window minimum calculations. This allows for efficient computation across most code files.</p>
  </li>
  <li>
    <p>The algorithm outputs the partition associated with dp $[k_{\text{min}},n]$, where $k_{\text{min}}=arg min_{\text{k}}(dp[k,n])$, as the most optimal partition.</p>
  </li>
</ol>

<h3 id="pretraining-with-span-corruption">Pretraining with Span Corruption</h3>

<ul>
  <li>
    <p>AST-T5’s pretraining is based on <em>span corruption</em>, a well-established method for pretraining transformer encoder-decoder models.</p>
  </li>
  <li>
    <p>In this approach, 15% of the input tokens are randomly masked and replaced by unique “sentinel” tokens, distinct within each example. Each unique sentinel token is associated with a specific ID and added to the model’s vocabulary.</p>
  </li>
  <li>
    <p>During pretraining, the encoder processes the corrupted input sequence. The decoder’s objective is to reconstruct the dropped-out tokens based on the encoder’s output representations. Specifically, the target sequence consists of the masked spans of tokens, demarcated by their corresponding sentinel tokens. This framework effectively trains the model to recover the oiginal text from a corrupted input.</p>
  </li>
</ul>

<h3 id="ast--aware-subtree-corruption">AST- Aware Subtree Corruption</h3>

<ul>
  <li>
    <p>AST-T5 augments the traditional span corruption paradigm by incorporating AST-awareness. Rather than arbitrarily masking consecutive token spans, AST-T5 masks code spans correspondiing to AST subtrees, rangining from individual expressions to entire function bodies.</p>
  </li>
  <li>
    <p><strong>Subtree Masking</strong>. It uses a recursive algorithm, to traverse the AST and select subtrees for masking.</p>
  </li>
  <li>
    <p>fulfills 2 goals:</p>
  </li>
</ul>

<ol>
  <li>
    <p>Introduce sufficient randomness across training epochs to enhance generalization.</p>
  </li>
  <li>
    <p>Control the masking granularity via a tunable hyperparameter $\theta$.</p>
  </li>
</ol>

<ul>
  <li>
    <p>The <strong>“mask quota”</strong> <em>m</em> denotes the number of tokens to be masked in a subtree rooted at node <em>t</em>. The size of a subtree corresponds to the number of tokens it encompasses, derived from the cumulative sizes of its children. For larger subtrees that exceed the size threshold $\theta$, masking is applied recursively.</p>
  </li>
  <li>
    <p>Meanwhile, smaller subtrees undergo a weighted shuffle, and the quota <em>m</em> is then apportioned among <em>t</em>’s children in a greedy fashion according to a shuffled order. The weights for shuffling are determined by a heuristic function on the size of each child, such that masking probabilities are distributed uniformly across leaf nodes.</p>
  </li>
  <li>
    <p>The parameter $\theta$ controls the granularity of masking. $\theta$ is randomly sampled within a predefined range(5-100) for each training example. This allows the pretraining framework to inherently accomodate tasks as varied as single-token completion to full function body generation from a given signature.</p>
  </li>
  <li>
    <p>The masking technique aims at the masking of ASt subtrees, potentially encompassing up to 100 tokens.
<img src="/notes/assets/img/posts/image-9.png" alt=""></p>
  </li>
  <li>
    <p><strong>Pretraining objective</strong>: Once subtrees are selected for masking and replaced with sentinel tokens, the encoder processes this modified input. Subsequently the decoder is tasked with reconstructing the original tokens within the masked subtrees.</p>
  </li>
</ul>

<h2 id="experimental-setup">Experimental Setup</h2>

<h3 id="model-architecture">Model Architecture</h3>

<ul>
  <li>12-layer encoder and a 12-layer decoder where each layer has 768 dimensions and 12 attention heads. total parameters: 277M</li>
</ul>

<h3 id="pretraining">Pretraining:</h3>

<ul>
  <li>
    <p>pretrained on a subset of The Stack Dedup corpus, a near-duplicated version of The Stack-a 3.1TB, spanning 358 programming languages.</p>
  </li>
  <li>
    <p>For this experiment, AST-T5’s training involves Python, Java, C, C++, C#, Markdown, and reStructuredText subsets, comprising a 588GB dataset with 93M code and natural language files.</p>
  </li>
  <li>
    <p>Each file is first parsed using the tree-Sitter multi-language parser and then tokenized with byte-level Byte-Pair Encoding using a byte-level BPE token vocabulary.</p>
  </li>
  <li>
    <p>Following AST-Aware segmentation, these files are partitioned into chunks of 1024 tokens.</p>
  </li>
  <li>
    <p>the model is pretrained using the AST-Aware Subtree Corruption objective for 524 billion tokens(1024 tokens per sequence,1024 sequences per batch and 500k steps)</p>
  </li>
  <li>
    <p>Pretraining uses Pytorch,Fairseq, FlashAttention conducted on 8 nodes, each with 8x NVIDIA A100 40 GB GPUs.</p>
  </li>
</ul>

<h2 id="evaluation">Evaluation</h2>

<ul>
  <li>evaluated across 3 tasks
    <ol>
      <li>text-to-code generation</li>
      <li>code-to-code transpilation</li>
      <li>Code classification</li>
    </ol>
  </li>
</ul>

<p><img src="/notes/assets/img/posts/image-10.png" alt=""></p>

<p><img src="/notes/assets/img/posts/image-11.png" alt=""></p>

<h2 id="results">Results</h2>

<p><img src="/notes/assets/img/posts/image-12.png" alt=""></p>

<p><img src="/notes/assets/img/posts/image-13.png" alt=""></p>

<h2 id="references">References</h2>

<ul>
  <li>
<a href="https://arxiv.org/pdf/2401.03003" rel="external nofollow noopener" target="_blank">AST-T5</a>
-<a href="https://github.com/%0Agonglinyuan/ast%20t5" rel="external nofollow noopener" target="_blank">github model repo</a>
</li>
</ul>

    </div>
  </article>

  

  

  
    
  

  
  
</div>
</div>
          </div>
        
      
    </div>

    <!-- Footer -->
    


  <footer class="sticky-bottom mt-5" role="contentinfo">
    

    <div class="container">
      
  © Copyright 2025
  Neural
  
  Notes. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.

  
  
    Last updated: November 27, 2025.
  

    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="/notes/assets/js/bootstrap.bundle.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>


  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="" integrity="" crossorigin="anonymous"></script>
  <script defer src="/notes/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script>























  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/notes/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>



  <!-- Sidebar Table of Contents -->
  <script defer src="/notes/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script>




<!-- Load Common JS -->
<script src="/notes/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/notes/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script>
<script defer src="/notes/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/notes/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>

<!-- Badges -->




  <!-- MathJax -->
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
  
    <script src="/notes/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script>
    <script defer src="" crossorigin="anonymous"></script>
  









  <!-- Scrolling Progress Bar -->
  <script defer src="/notes/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script>







  <!-- Back to Top -->
  <script src="/notes/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>



  <!-- Search -->
  <script type="module" src="/notes/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script>
  <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys>
  <script src="/notes/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script>
  <script src="/notes/assets/js/search-data.js"></script>
  <script src="/notes/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script>




  </body>
</html>
