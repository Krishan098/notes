# RAG 
- Q&A chatbots are applications that can answer questions about specific source information.

- These applications use a technique known as Retreival Augmented Generation.

- Two main components:

1. Indexing: a pipeline for ingesting data from a source and indexing it.

- Load: first we need to load our data, done using document loaders

- split: text splitters break large Documents into smaller chunks. This is useful for both indexing data and passing it into a model.

- store: we need to store and index our splits, so they can be searched over later. use vectorstore and embeddings model.

2. Retrieval and generation:

- Retrieve: Given a user input, relevant splits are retrieved form storage using a retriever.

- Generate: A chatmodlel/llm produces an answer using a prompt that includes both the question with the retrieved data.


## indexing

### loading documents

- we use DocumentLoaders.

### splitting documents

- models can struggle to find information in very long inputs.

- To handle this we'll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.

- RecursiveCharacterTextSplitter will recursively split the document using common separators like new lines until each chunk is the appropriate size.

- TextSplitter : Object that splits a list of Documents into smaller chunks.

- DocumentTransformer: Object that performs a transformation on a list of Document objects.

### Storing documents

- we embed the contents of each document split and insert these embeddings into a vector store.

- we can then use vector search to retrieve relevant documents.

- Embeddings: wrapper around a text embedding model, used for converting text to embeddings.

- VectorStore: wrapper around a vector database, used for storing and querying embeddings.

## Retrieval and Generation

- to use langgraph , we need to define three things:

1. state of our application

2. The nodes of our application

3. The control flow of our application

### State:

- the state of our application controls what data is input to the application, transferred between the steps and output by the application.


- Our retrieval step simply runs a similarity search using the input question and generation step formats the retrieved context and original question into a prompt for the chat model.

## Query Analysis

- employs models to transform or construct optimized search queries from raw user input. 

============================== PART2 ====================================

- accomodates converstion style interactions and multi-step retrieval processes.

- This involves managing a chat history.

## Two approaches

1. Chains: we execute at most one retrieval step

2. Agents: we give an LLM discretion to execute multiple retrieval steps

- Conversational experiences can be naturally represented using a sequence of messages.

- In addition to messages from the user and assistant, retrieved documents and other artifacts can be incorporated into a message sequence via tool messages.

### Chains
- We have:

1. User input as a HumanMessage

2. Vector store query as an AIMessage with tool calls

3. Retrieved documents as a ToolMessage

4. Final response as a AIMessage

- Leveraging tool-calling to interact with a retrieval step has another benefit, which is that the query for the retrieval is generated by our model. 

- Our graph consists of 3 nodes:

1. A node that fields the user input, either generating a query for the retriever or responding directly.

2. A node for the retriever tool that executes the retrival step

3. A node that generates the final response using the retrieved context.

- Finally, we compile our application into a single graph object.

- LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.

- To manage multiple conversational turns and threads, all we have to do is specify a checkpointer when compiling our application. Because the nodes in our graph are appending messages to the state, we will retain a consistent chat history across invocations.

### Agents
- Agents leverage the reasoning capabilities of LLMs to make decision during execution. Using agents allows you to offload additional discretion over the retrieval process. 

- able to execute multiple retrieval steps in service of a query, or iterate on a single search.

- The key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information.

## Key concepts

![alt text](image-10.png)

1. Retrieval system: Retrieve relevant information from a knowledge base.

2. Adding external knowledge: Pass retrieved information to a model.

- offers several advantages:


- Up-to-date information: RAG can access and utilize the latest data, keeping responses current.
- Domain-specific expertise: With domain-specific knowledge bases, RAG can provide answers in specific domains.
- Reduced hallucination: Grounding responses in retrieved facts helps minimize false or invented information.
- Cost-effective knowledge integration: RAG offers a more efficient alternative to expensive model fine-tuning.
